import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import threading
from PIL import Image

# Mediapipeã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
mp_pose = mp.solutions.pose
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
pose = mp_pose.Pose()
hands = mp_hands.Hands()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ¼ãƒ‰é¸æŠ
main_mode = st.sidebar.radio("ğŸ” æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", ["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º", "å†™çœŸã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡º", "å‹•ç”»ã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡º"], key="main_mode_key")
sub_mode = st.sidebar.radio("ğŸ“Œ æ¤œå‡ºå¯¾è±¡ã‚’é¸æŠ", ["ä½“ã®é–¢ç¯€ã®ã¿", "æ‰‹ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"], key="sub_mode_key")

st.title("ğŸ“Œ éª¨æ ¼æ¤œå‡ºã‚¢ãƒ—ãƒª")

# **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰**
if main_mode == "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º":
    st.subheader("ğŸ¥ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§éª¨æ ¼ã‚’æ¤œå‡ºä¸­...")
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ãƒãƒƒãƒ•ã‚¡ã‚’æœ€å°åŒ–
    FRAME_WINDOW = st.empty()
    
    def process_video():
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (640, 480), interpolation=cv2.INTER_AREA)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results_pose = pose.process(frame) if sub_mode in ["ä½“ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
            results_hands = hands.process(frame) if sub_mode in ["æ‰‹ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
            if results_pose and results_pose.pose_landmarks:
                mp_drawing.draw_landmarks(frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)
            if results_hands and results_hands.multi_hand_landmarks:
                for hand_landmarks in results_hands.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            FRAME_WINDOW.image(frame, channels="RGB")
        cap.release()
    
    thread = threading.Thread(target=process_video)
    thread.start()

# **å†™çœŸã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡ºã™ã‚‹ãƒ¢ãƒ¼ãƒ‰**
elif main_mode == "å†™çœŸã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡º":
    st.subheader("ğŸ“¸ å†™çœŸã‚’æ’®å½± or ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    img_file = st.file_uploader("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["jpg", "jpeg", "png"])
    if img_file:
        image = Image.open(img_file)
        frame = np.array(image)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        results_pose = pose.process(frame) if sub_mode in ["ä½“ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
        results_hands = hands.process(frame) if sub_mode in ["æ‰‹ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
        if results_pose and results_pose.pose_landmarks:
            mp_drawing.draw_landmarks(frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)
        if results_hands and results_hands.multi_hand_landmarks:
            for hand_landmarks in results_hands.multi_hand_landmarks:
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        st.image(frame, channels="BGR", use_column_width=True)

# **å‹•ç”»ã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡ºã™ã‚‹ãƒ¢ãƒ¼ãƒ‰**
elif main_mode == "å‹•ç”»ã‹ã‚‰åº§æ¨™ã‚’æ¤œå‡º":
    st.subheader("ğŸï¸ å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    video_file = st.file_uploader("å‹•ç”»ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["mp4", "mov", "avi"])
    if video_file:
        temp_video_path = "temp_video.mp4"
        with open(temp_video_path, "wb") as f:
            f.write(video_file.read())
        cap = cv2.VideoCapture(temp_video_path)
        FRAME_WINDOW = st.empty()
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (640, 480), interpolation=cv2.INTER_AREA)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results_pose = pose.process(frame) if sub_mode in ["ä½“ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
            results_hands = hands.process(frame) if sub_mode in ["æ‰‹ã®é–¢ç¯€ã®ã¿", "ã™ã¹ã¦"] else None
            if results_pose and results_pose.pose_landmarks:
                mp_drawing.draw_landmarks(frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)
            if results_hands and results_hands.multi_hand_landmarks:
                for hand_landmarks in results_hands.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            FRAME_WINDOW.image(frame, channels="RGB")
        cap.release()
